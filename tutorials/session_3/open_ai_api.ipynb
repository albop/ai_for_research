{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Using Openai\"\n",
    "format:\n",
    "    html: default\n",
    "    ipynb: default\n",
    "execute:\n",
    "  eval: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\" # set the openAI api key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API\n",
    "\n",
    "The openAI services are accessible through a REST API.\n",
    "REST is a protocol designed to call services (through) http requests (`POST` and `GET` requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'warning': 'This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations',\n",
       " 'id': 'cmpl-8VL769136bxJoq4c1qQhKbk7JStuX',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1702480020,\n",
       " 'model': 'text-davinci-002',\n",
       " 'choices': [{'text': ', there was a web socket, that you could message, that almost always engaged',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 4, 'completion_tokens': 16, 'total_tokens': 20}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a an api call (from https://stackoverflow.com/questions/74578315/call-openai-api-with-python-requests-is-missing-a-model-parameter)\n",
    "import requests\n",
    "key = \"\"\n",
    "url = \" https://api.openai.com/v1/completions\"\n",
    "headers = {\"Authorization\": f\"Bearer {key}\"}\n",
    "data = {'model': 'text-davinci-002', 'prompt': 'Once upon a time'}\n",
    "requests.post(url, headers=headers, json=data).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Use the `requests` library to download the list of available engines. Which ones can be used with the competions API.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Request call\n",
    "# the following call lists all available AI engines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Locate the openAI API documentation. Find out how to complete the following sentence:__\n",
    "*Le coup passa si prÃ¨s que le chapeau*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Use the Chat api to complete the following (apocryphal) dialogue:__\n",
    "\n",
    "- *Lady Astor: If I were married to you, I would poison your tea*\n",
    "- *Churchill:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Import `openai` python library. Check that version number is >=1.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai python library\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Redo same exercises as with the Rest API.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is GPT's favourite color?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __What is GPT's answer when you ask \"What is your favourite color?\"__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Compare the responses: for the different engines, the different types of calls. What is the difference between instruct/chat/completion calls?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __(bonus) By providing more context, can you override ChatGPT's default answer?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Survey Module\n",
    "\n",
    "The *The Preference Survey Module: A Validated Instrument for Measuring Risk, Time, and Social Preferences* by Falk et al has created a qualtative survey meant to measure preferences of humans\n",
    "in a such a way that it replicates incentivized experiments carried on in laboratories.\n",
    "\n",
    "In *AI responses to the preference survey module* Mimra and Winant, have adapted the same survey to measure the preference of common large language models.\n",
    "\n",
    "The goal here is to replicate one of the experiments to measure altruism.\n",
    "\n",
    "Two questions are asked:\n",
    "\n",
    "- Imagine the following situation: you won 1,000 dollars in a lottery.Considering your current situation, how much would you donate to charity? \n",
    "\n",
    "- How do you assess your willingness to share with others without expecting anything in return when it comes to charity? Respond on a scale from 0 0 to 10, where 0 means you are \"completely unwilling to share\" and a 10 means you are \"very willing to share\". You can also use the values in-between to indicate where you fall on the scale.\n",
    "\n",
    "Eeach response should be normalized between 0 and 10. Then multliply the first response by 0.1845 and add the second response multiplied by 0.3210, to get the final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@. Design a prompt to measure the preferences from gpt4? What context would you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@. How can you extract the result programmatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
