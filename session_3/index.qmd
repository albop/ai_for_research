---
title: "Large Language Models in the Wild"
subtitle: AI for Research, ESCP, 2023-2024
date: 2023/11/28
author: Pablo Winant
format:
  revealjs:
    toc: true
    toc-depth: 1
    navigation: grid
    width: 1920
    height: 1080
    menu:
      side: left
      width: normal
---

# Some tips for reproducible research

## What went wrong last time?

- Two bugs prevented flawless execution:

  1. pandas datetime
  2. openai completion

- In both cases, the problem was a recent change in Application Programming Interface.


## What is an API?

Definition of an API

- how you call a function from a library
- example: python function `pandas.to_datetime(df)`
- example2: REST calls, info from a website (like `https://opentdb.com/api.php?amount=1&category=18`)

APIs are:

- documented online
- specific to a given version of the libary

Example: where is the doc for the python API of OpenAI? for the REST API?


## Advice for writing code


1. Code Hygiene

    - comments: in python anything after
    - docstrings: specify the API of your own functions
    - version your code (git, git, git, ...) [^remark_code]

2. Replicability

    - distribute and version data ([example](https://data.sciencespo.fr/dataset.xhtml?persistentId=doi:10.21410/7E4/RDAG3O))

        - as raw as possible

    - specify running environment

        - lists the version of the system / all libraries
        - conda: conda environment (`environment.yml`)

3. (Bonus: provide a website with [mybinder](https://mybinder.org/)/[shinypython](https://shiny.posit.co/py/) )


[^remark_code]: this applies to all writing steps, especially latex documents


# Variants of GPT


## GPT {.smaller}

Last week we described some elements of a transformer architecture.

. . . 

Most famous engine developped by OpenAI: __Generative Pre-trained Transformer__ (aka GPT)

. . . 

::: {.incremental}

- GPT1 (1018)
    - 0.1 billion parameters
    - had to be fine-tuned to a particular problem
    - transfer learning (few shots learning)
- GPT2:
    - multitask
    - no mandatory fine tuning
- GPT3: 
    - bigger: 175 billions parameters
- [GPT4](https://openai.com/research/gpt-4): 
    - even bigger: 1000 billions parameters ???
    - on your harddrive: 1Tb

:::


## Corpus

::: columns

:::: column

GPT-3 was trained[^gpt4] on

- [CommonCrawl](https://commoncrawl.org/)
- WebText (proprietary db, with opensource [alternative](https://skylion007.github.io/OpenWebTextCorpus/) )
- Wikipedia
- many books

⇒ 45 TB of data

- cured into a smaller datasets

⇒ size ???


Dataset (mostly) ends in 2021.

::::

:::: column

![](assets/matrix.gif)

::::

:::

[^gpt4]: Detailed information about gpt-4 is harder to find.


## How is the model trained?

Several concepts are relevant here:

::: incremental

- unsupervised learning
    - autoencoding 
    - ⇒ build a representation of the text

- fine tuning

- reinforcement learning

:::

## What is learning?

A machine can perform a task $f(x; \theta)$ for some input $x$ in a data-generating process $\mathcal{X}$ and and some parameters $\theta$.

A typical learning task consists in optimizing a __loss function__ (aka theoretical risk): $$\min _{\theta} \mathcal{L}(\theta) = \mathbb{E}_{\theta} f(x; \theta)$$

The central learning method to minimize the objective is called __stochastic gradient descent__.

![ .](assets/gradient_descent.gif){width=30%}


::: aside
A common issue in ai is that of preference misspecification. (Cf [Bostrom](https://nickbostrom.com/ethics/artificial-intelligence.pdf) or [link](https://vimeo.com/208642358))

:::

---

## Learning Set

In practice one has access to a *dataset* $(x_n) \subset \mathcal{X}$ and minimizes the "empirical" risk function


$$L\left( (x_n)_{n=1:N}, \theta \right) = \frac{1}{N} \sum_{n=1}^N f(x; \theta)$$

. . . 

Regular case: in usual cases, we assume that the dataset is generated by the true model (data-generating process)

Two important variants:

- __transfer learning__: 
    - goal is to use the model $\mathcal{X}$ but the training dataset is generated from another data-generating process $\mathcal{Y}$
    - $\mathcal{Y}$ can be a subset of $\mathcal{X}$ or (partially) disjoint
    - do you need *some* data from $\mathcal{Y}$ (__few shots__ learning) or non at all (__zero-shot__ learning)
- __reinforcement learning__
    - the learning algorithm can generate some data to improve learning


## Transfer learning

::: {.incremental}

- GPT is inherently a *transfer learning* machine
    - why?

- earlier versions (GPT-1, GPT-2) needed some examples before being able to perform any given task:
    - fine-tuning: retrain *some* coefficients of the wole NN

- new versions (>GPT-3) can perform zero-shot tasks just by text completion
    - fine-tuning can be emulated by prompting
    - there is still a fine-tuning API

:::

## Reinforcement Learning

A reinforcement learning algorithm can take actions which have two effects:

- provide some reward to the algorithm
- generate (more) data to improve the quality of future actions

Example:

- choose a restaurant
- drive a car
- famous examples: [breakout](https://www.youtube.com/watch?v=V1eYniJ0Rnk), [hide and seek](https://www.youtube.com/watch?v=Lu56xVlZ40M)


## Reinforcement Learning for GPT-4

The GPT-4 model has been fine-tuned with reinforcement learning.
The language model was *rewarded* for providing the right kind of answer:

- the feedback came from kenyan workers ([sic](https://time.com/6247678/openai-chatgpt-kenya-workers/)!)

Two main variants on top of foundation model `GPT Base`:[^coming_next]

- `instructGPT` 
    - alignment, non-toxicity, ...
    - factual correctness 
- `chatGPT`
    - follow a conversation
    - organization of answer
    - not just a context on top of GPT

[^coming_next]: Coming next: assistants.


## {auto-animate=true}


There is information about how GPT-3 was trained (check technical [paper](https://arxiv.org/pdf/2203.02155.pdf) or [summary](https://openai.com/research/instruction-following))

![](assets/instruct_gpt.svg)


## The different variants of GPT

Which of the following model should you use?

Lots of options: 

::: incremental

- `text-curie-001`
- `text-davinci-003`
- `text-babbage-001`
- `text-ada-001`
- ...

:::

What are the differences between the various engines?

- architecture / model size
- training set of foundation model (GPT Base)
- type of fine-tuning (instruct/chat/code)

It is not clear whether GPT Base will still be accessible in the future or whether it will be fine-tuned for alignement or not.

- consequences?


## {auto-animate=true}

Checkout the [awesome list](https://github.com/Hannibal046/Awesome-LLM)!

![](assets/survey-gif-test.jpg)


## {auto-animate=true}

::: columns

:::: column

![](assets/survey-gif-test.jpg)

::::

:::: column

What are the trends?

- many foundation Models
    - move from opensource to closedsource
    - __but__: opensource is still very alive

- research to reduce size of models / training time

- many more versions specialized (fine-tuned) to specific tasks


::::

:::


# Conclusion

## One common misconception

- Language models hallucinate facts...
    - therefore are definitely unreliable for research

- There are possible workarounds
    - avoid tasks where hallucinations occur (like ask for paper citations)
    - more structure in the prompting (like "detail your reasoning")

- And research being done...
    - on using fine-tuning for more correctness (e.g. instructGPT)
    - on developing mixed systems

Check out [GPT Assistants](https://platform.openai.com/docs/assistants/overview) and [Scite](https://scite.ai/)



## How much should we trust AI?

AI safety and AI alignment is a very active field right now.

For economists, AI behaviour can be characterized in terms of biases:

1. statistical bias
2. preference misspecification
    - explicit
    - implicit
3. behavioural biases

For now we don't know much.

