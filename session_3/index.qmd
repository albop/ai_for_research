---
title: "Large Language Models in the Wild"
subtitle: AI for Research, ESCP, 2023-2024
date: 2023/11/28
author: Pablo Winant
format:
  revealjs:
    toc: true
    toc-depth: 1
    navigation: grid
    width: 1920
    height: 1080
    menu:
      side: left
      width: normal
---

# Some tips for reproducible research

## What went wrong last time?

- Two bugs prevented flawless execution:

  1. pandas datetime
  2. openai completion

- In both cases, the problem was a recent change in Application Programming Interface.


## What is an API?

Definition of an API

- how you call a function from a library
- example: python function `pandas.to_datetime(df)`
- example2: REST calls, info from a website (like `https://opentdb.com/api.php?amount=1&category=18`)

APIs are:

- documented online
- specific to a given version of the libary

Example: where is the doc for the python API of OpenAI? for the REST API?


## Advice for writing code


1. Code Hygiene

    - comments: in python anything after
    - docstrings: specify the API of your own functions
    - version your code (git, git, git, ...) [^remark_code]

2. Replicability

    - distribute and version data ([example](https://data.sciencespo.fr/dataset.xhtml?persistentId=doi:10.21410/7E4/RDAG3O))

        - as raw as possible

    - specify running environment

        - lists the version of the system / all libraries
        - conda: conda environment (`environment.yml`)

3. (Bonus: provide a website with [mybinder](https://mybinder.org/)/[shinypython](https://shiny.posit.co/py/) )


[^remark_code]: this applies to all writing steps, especially latex documents


# Variants of GPT


## GPT {.smaller}

Last week we described some elements of a transformer architecture.

. . . 

Most famous engine developped by OpenAI: __Generative Pre-trained Transformer__ (aka GPT)

. . . 

::: {.incremental}

- GPT1 (1018)
    - 0.1 billion parameters
    - had to be fine-tuned to a particular problem
    - transfer learning (few shots learning)
- GPT2:
    - multitask
    - no mandatory fine tuning
- GPT3: 
    - bigger: 175 billions parameters
- [GPT4](https://openai.com/research/gpt-4): 
    - even bigger: 1000 billions parameters ???
    - on your harddrive: 1Tb

:::


## Corpus

::: columns

:::: column

GPT-3 was trained[^gpt4] on

- [CommonCrawl](https://commoncrawl.org/)
- WebText (proprietary db, with opensource [alternative](https://skylion007.github.io/OpenWebTextCorpus/) )
- Wikipedia
- many books

⇒ 45 TB of data

- cured into a smaller datasets

⇒ size ???


Dataset (mostly) ends in 2021.

::::

:::: column

![](assets/matrix.gif)

::::

:::

[^gpt4]: Detailed information about gpt-4 is harder to find.


## How is the model trained?

Several concepts are relevant here:

::: incremental

- unsupervised learning
    - autoencoding 
    - ⇒ build a representation of the text

- fine tuning

- reinforcement learning

:::

## What is learning?

A machine can perform a task $f(x; \theta)$ for some input $x$ in a data-generating process $\mathcal{X}$ and and some parameters $\theta$.

A typical learning task consists in optimizing a __loss function__ (aka theoretical risk): $$\min _{\theta} \mathcal{L}(\theta) = \mathbb{E}_{\theta} f(x; \theta)$$

The central learning method to minimize the objective is called __stochastic gradient descent__.

![ .](assets/gradient_descent.gif){width=30%}


::: aside
A common issue in ai is that of preference misspecification. (Cf [Bostrom](https://nickbostrom.com/ethics/artificial-intelligence.pdf) or [link](https://vimeo.com/208642358))

:::

---

## Learning Set

In practice one has access to a *dataset* $(x_n) \subset \mathcal{X}$ and minimizes the "empirical" risk function


$$L\left( (x_n)_{n=1:N}, \theta \right) = \frac{1}{N} \sum_{n=1}^N f(x; \theta)$$

. . . 

Regular case: in usual cases, we assume that the dataset is generated by the true model (data-generating process)

Two important variants:

- __transfer learning__: 
    - goal is to use the model $\mathcal{X}$ but the training dataset is generated from another data-generating process $\mathcal{Y}$
    - $\mathcal{Y}$ can be a subset of $\mathcal{X}$ or (partially) disjoint
    - do you need *some* data from $\mathcal{Y}$ (__few shots__ learning) or non at all (__zero-shot__ learning)
- __reinforcement learning__
    - the learning algorithm can generate some data to improve learning


## Transfer learning

::: {.incremental}

- GPT is inherently a *transfer learning* machine
    - why?

- earlier versions (GPT-1, GPT-2) needed some examples before being able to perform any given task:
    - fine-tuning: retrain *some* coefficients of the wole NN

- new versions (>GPT-3) can perform zero-shot tasks just by text completion
    - fine-tuning can be emulated by prompting
    - there is still a fine-tuning API

:::

## Reinforcement Learning

A reinforcement learning algorithm can take actions which have two effects:

- provide some reward to the algorithm
- generate (more) data to improve the quality of future actions

Example:

- choose a restaurant
- drive a car
- famous exapmles: [breakout](https://www.youtube.com/watch?v=V1eYniJ0Rnk), [hide and seek](https://www.youtube.com/watch?v=Lu56xVlZ40M)


## Reinforcement Learning for GPT-4

The GPT-4 model has been fine-tuned with reinforcement learning.
The language model was *rewarded* for providing the right kind of answer:

- the feedback came from kenyan workers ([sic](https://time.com/6247678/openai-chatgpt-kenya-workers/)!)

Two main variants:[^coming_next]

- `instructGPT` 
    - alignment, non-toxicity, ...
    - factual correctness 
- `chatGPT`
    - follow a conversation
    - organization of answer
    - not just a context on top of GPT


[^coming_next]: Coming next: assistants.

## The different variants of GPT

Which of the following model should you use?

Lots of options: 

::: incremental

- `text-curie-001`
- `text-davinci-003`
- `text-babbage-001`
- `text-ada-001`
- ...

:::

What are the differences between the various engines?


# Some examples of applications


## Replace Many NLP Algorithms

Beyond generating text, most Natural Language Processing tasks can be now done with GPT-3:

- Named entity recognition
- Classification
    - sentiment analysis
    - multimodel sentiment analysis
- Entity linking
- Summarization
- Many [more](https://platform.openai.com/examples)

## Named Entity recognition

Prompt:[^cf]

```
[Text]: Helena Smith founded Core.ai 2 years ago. She is now the CEO and CTO of the company and is building a team of highly skilled developers in machine learning and natural language processing.
[Position]: CEO and CTO
###
[Text]: Tech Robotics is a robot automation company specialized in AI driven robotization. Its Chief Technology Officer, Max Smith, says a new wave of improvements should be expected for next year.
[Position]: Chief Technology Officer
###
[Text]: François is a Go developer. He mostly works as a freelancer but is open to any kind of job offering!
[Position]: Go developer
###
[Text]: Maxime is a data scientist at Auto Dataset, and he's been working there for 1 year.
[Position]:
```
. . .

```Response: Data Scientist```

[^cf]: https://towardsdatascience.com/advanced-ner-with-gpt-3-and-gpt-j-ce43dc6cdb9c


## Fine-tuning (1)

- To get better result, or to proceed larger amounts of data, pretrain the model

```{{python}}
model.train("""
"[Text]: Helena Smith founded Core.ai 2 years ago. She is now the CEO and CTO of the company and is building a team of highly skilled developers in machine learning and natural language processing.
[Position]: CEO and CTO
""")
```

```{{python}}
model.train("""
[Text]: Tech Robotics is a robot automation company specialized in AI driven robotization. Its Chief Technology Officer, Max Smith, says a new wave of improvements should be expected for next year.
[Position]: Chief Technology Officer
""")
```

## Fine-tuning (2)

- Query the model:
```{{python}}
model.eval("""
[Text]: Maxime is a data scientist at Auto Dataset, and he's been working there for 1 year.
[Position]:
""")
```

...

```
[Text]: Maxime is a data scientist at Auto Dataset, and he's been working there for 1 year.
[Position]: Data Scientist```
```

## Sentiment Analysis

Zero-shot learning:

```
Decide whether a Tweet's sentiment is positive, neutral, or negative.

Tweet: "I loved the new Batman movie!"
Sentiment:
```

Output: ```Positive```


# Conclusion


## Homework for next time


- finish tutorial / pushups
- start the work on your replication coursework
- propose a topic for your talk in the last session (20 min)
